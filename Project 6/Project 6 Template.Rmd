---
title: 'Project 6: Randomization and Matching'
author: "Miray Salman"
date: "April 2024"
output: pdf_document
---

# Introduction

In this project, you will explore the question of whether college education causally affects political participation. Specifically, you will use replication data from \href{https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1409483}{Who Matches? Propensity Scores and Bias in the Causal Eï¬€ects of Education on Participation} by former Berkeley PhD students John Henderson and Sara Chatfield. Their paper is itself a replication study of \href{https://www.jstor.org/stable/10.1017/s0022381608080651}{Reconsidering the Effects of Education on Political Participation} by Cindy Kam and Carl Palmer. In their original 2008 study, Kam and Palmer argue that college education has no effect on later political participation, and use the propensity score matching to show that pre-college political activity drives selection into college and later political participation. Henderson and Chatfield in their 2011 paper argue that the use of the propensity score matching in this context is inappropriate because of the bias that arises from small changes in the choice of variables used to model the propensity score. They use \href{http://sekhon.berkeley.edu/papers/GenMatch.pdf}{genetic matching} (at that point a new method), which uses an approach similar to optimal matching to optimize Mahalanobis distance weights. Even with genetic matching, they find that balance remains elusive however, thus leaving open the question of whether education causes political participation.

You will use these data and debates to investigate the benefits and pitfalls associated with matching methods. Replication code for these papers is available online, but as you'll see, a lot has changed in the last decade or so of data science! Throughout the assignment, use tools we introduced in lab from the \href{https://www.tidyverse.org/}{tidyverse} and the \href{https://cran.r-project.org/web/packages/MatchIt/MatchIt.pdf}{MatchIt} packages. Specifically, try to use dplyr, tidyr, purrr, stringr, and ggplot instead of base R functions. While there are other matching software libraries available, MatchIt tends to be the most up to date and allows for consistent syntax.

# Data

The data is drawn from the \href{https://www.icpsr.umich.edu/web/ICPSR/studies/4023/datadocumentation#}{Youth-Parent Socialization Panel Study} which asked students and parents a variety of questions about their political participation. This survey was conducted in several waves. The first wave was in 1965 and established the baseline pre-treatment covariates. The treatment is whether the student attended college between 1965 and 1973 (the time when the next survey wave was administered). The outcome is an index that calculates the number of political activities the student engaged in after 1965. Specifically, the key variables in this study are:

\begin{itemize}
    \item \textbf{college}: Treatment of whether the student attended college or not. 1 if the student attended college between 1965 and 1973, 0 otherwise.
    \item \textbf{ppnscal}: Outcome variable measuring the number of political activities the student participated in. Additive combination of whether the student voted in 1972 or 1980 (student\_vote), attended a campaign rally or meeting (student\_meeting), wore a campaign button (student\_button), donated money to a campaign (student\_money), communicated with an elected official (student\_communicate), attended a demonstration or protest (student\_demonstrate), was involved with a local community event (student\_community), or some other political participation (student\_other)
\end{itemize}

Otherwise, we also have covariates measured for survey responses to various questions about political attitudes. We have covariates measured for the students in the baseline year, covariates for their parents in the baseline year, and covariates from follow-up surveys. \textbf{Be careful here}. In general, post-treatment covariates will be clear from the name (i.e. student\_1973Married indicates whether the student was married in the 1973 survey). Be mindful that the baseline covariates were all measured in 1965, the treatment occurred between 1965 and 1973, and the outcomes are from 1973 and beyond. We will distribute the Appendix from Henderson and Chatfield that describes the covariates they used, but please reach out with any questions if you have questions about what a particular variable means.

```{r}
update.packages(ask = FALSE)  # Update all packages without asking for confirmation

#install.packages("rmarkdown")
#install.packages("knitr")
#install.packages("tinytex")
#install.packages("ggplot2")

# Load tidyverse and MatchIt
# Feel free to load other libraries as you wish
library(tidyverse)
library(MatchIt)

# Load ypsps data
ypsps <- read_csv('data/ypsps.csv')
head(ypsps)
```

# Randomization

Matching is usually used in observational studies to to approximate random assignment to treatment. But could it be useful even in randomized studies? To explore the question do the following:

\begin{enumerate}
    \item Generate a vector that randomly assigns each unit to either treatment or control
    \item Choose a baseline covariate (for either the student or parent). A binary covariate is probably best for this exercise.
    \item Visualize the distribution of the covariate by treatment/control condition. Are treatment and control balanced on this covariate?
    \item Simulate the first 3 steps 10,000 times and visualize the distribution of treatment/control balance across the simulations.
\end{enumerate}

```{r}
colnames(ypsps)
```

```{r}
# Now, I wanna explore the covariate I chose a little bit (i.e. is it numeric? Binary?)
table(ypsps$student_vote) #Yeyy, the variable is binary!
class(ypsps$student_vote)
summary(ypsps$student_vote)
```
```{r}
# Generate a vector that randomly assigns each unit to treatment/control
set.seed(123) 
random_assignment <- sample(c(0, 1), size = nrow(ypsps), replace = TRUE)
ypsps$treatment_random <- random_assignment  # Adding it to my df

# Choose a baseline covariate (use dplyr for this)
# I'm choosing "student_vote". Now, I am converting the student_vote column to numeric, and saving the modified dataframe back into ypsps. This was not really necessary as student_vote is already numeric, but I wasn't sure what else to do here with dplyr.
library(dplyr)
ypsps <- ypsps %>%
  mutate(student_vote = as.numeric(student_vote)) 

# Visualize the distribution by treatment/control (ggplot)
set.seed(123)
library(ggplot2)
ggplot(ypsps, aes(x = as.factor(treatment_random), fill = as.factor(student_vote))) +
  geom_bar(position = "fill") +
  scale_fill_manual(values = c("0" = "blue", "1" = "red"),
                    labels = c("Did Not Vote", "Voted")) +
  labs(x = "Treatment Assignment", y = "Proportion", fill = "student_vote") +
  theme_minimal()

```
```{r}
# My graph shows that both the treatment (1) and untreated (0) groups are perfectly balanced, having the same proportion of those who voted and those who did not (from the student_vote variable). This is highly unusual. First, I want to investigate whether they are truly perfectly balanced, because visually, it looks like there might be a small difference.
ypsps %>%
  group_by(treatment_random) %>%
  summarise(Proportion_Voted = mean(as.numeric(student_vote)))
```

```{r}
# Upon further inspection, I was able to identify that the proportions though being very close to each other are not exactly the same.
```

```{r}
# Simulate this 10,000 times (monte carlo simulation - see R Refresher for a hint)
balance_diffs <- replicate(10000, {
  random_assignment <- sample(c(0, 1), size = nrow(ypsps), replace = TRUE)
  treatment_group <- ypsps$student_vote[random_assignment == 1]
  control_group <- ypsps$student_vote[random_assignment == 0]
  prop_treat <- mean(treatment_group)
  prop_control <- mean(control_group)
  prop_treat - prop_control
})

# Vis the balance of the differences
ggplot(data.frame(BalanceDifference = balance_diffs), aes(x = BalanceDifference)) +
  geom_histogram(bins = 50, color = "black", fill = "skyblue") +
  labs(x = "Difference in Proportions", y = "Frequency") +
  theme_minimal()
```

## Questions
\begin{enumerate}
    \item \textbf{What do you see across your simulations? Why does independence of treatment assignment and baseline covariates not guarantee balance of treatment assignment and baseline covariates?}
\end{enumerate}

My Answer: The distribution of the differences in proportions of the baseline covariate student_vote between the treatment and control groups is balanced across my simulations. In accordance with the Central Limit Theorem, the difference in proportions has a normal distribution, which can be explained by the large number of simulations.
Independence of treatment assignment does not guarantee balance, because in small samples, random assignment can still result in imbalances simply due to chance. This is due to the Law of Large Numbers, which indicates that the average treatment effect will converge to the true effect with increasing number of simulations, also leading the distribution of the covariate to balance out. 


# Propensity Score Matching

## One Model
Select covariates that you think best represent the "true" model predicting whether a student chooses to attend college, and estimate a propensity score model to calculate the Average Treatment Effect on the Treated (ATT). Plot the balance of the top 10 (or fewer if you select fewer covariates). Report the balance of the p-scores across both the treatment and control groups, and using a threshold of standardized mean difference of p-score $\leq .1$, report the number of covariates that meet that balance threshold.


```{r}
library(MatchIt)

treatment_var <- "college"
covariates <- c("student_GPA", "student_FamTalk", "parent_EducHH", 
                "student_PubAff", "student_Race", "student_EgoA", 
                "parent_HHInc", "student_PID", "student_FrTalk", 
                "parent_OwnHome")

# Fit propensity score model 
formula_ps <- as.formula(paste(treatment_var, "~", paste(covariates, collapse = " + ")))
model_ps <- glm(formula_ps, family = binomial(), data = ypsps)

# Check summary of the model to evaluate coefficients
summary(model_ps)

# Add estimated propensity scores to the df
ypsps <- ypsps %>% mutate(propensity_score = predict(model_ps, type = "response"))


```

```{r}
# 1. Estimate propensity scores and calculate ATT
match_model <- matchit(formula_ps, data = ypsps, method = "nearest")

# 2. Plot balance of covariates before and after matching
library(cobalt)
bal.plot(match_model)

# 3. Report balance of propensity scores
balance_stats <- bal.tab(match_model, un = TRUE, display = FALSE)
summary(balance_stats)

# 4. Determine the number of covariates meeting balance threshold
num_balanced_covariates <- sum(balance_stats$balance$std.diff <= 0.1)

# Print the number of balanced covariates
cat("Number of covariates meeting balance threshold:", num_balanced_covariates, "\n")
```

## Simulations

Henderson/Chatfield argue that an improperly specified propensity score model can actually \textit{increase} the bias of the estimate. To demonstrate this, they simulate 800,000 different propensity score models by choosing different permutations of covariates. To investigate their claim, do the following:

\begin{itemize}
    \item Using as many simulations as is feasible (at least 10,000 should be ok, more is better!), randomly select the number of and the choice of covariates for the propensity score model.
    \item For each run, store the ATT, the proportion of covariates that meet the standardized mean difference $\leq .1$ threshold, and the mean percent improvement in the standardized mean difference. You may also wish to store the entire models in a list and extract the relevant attributes as necessary.
    \item Plot all of the ATTs against all of the balanced covariate proportions. You may randomly sample or use other techniques like transparency if you run into overplotting problems. Alternatively, you may use plots other than scatterplots, so long as you explore the relationship between ATT and the proportion of covariates that meet the balance threshold.
    \item Finally choose 10 random models and plot their covariate balance plots (you may want to use a library like \href{https://cran.r-project.org/web/packages/gridExtra/index.html}{gridExtra} to arrange these)
\end{itemize}

\textbf{Note: There are lots of post-treatment covariates in this dataset (about 50!)! You need to be careful not to include these in the pre-treatment balancing. Many of you are probably used to selecting or dropping columns manually, or positionally. However, you may not always have a convenient arrangement of columns, nor is it fun to type out 50 different column names. Instead see if you can use dplyr 1.0.0 functions to programatically drop post-treatment variables (\href{https://www.tidyverse.org/blog/2020/03/dplyr-1-0-0-select-rename-relocate/}{here} is a useful tutorial).}

```{r}
  # Remove post-treatment covariates
library(MatchIt)
library(cobalt)
library(dplyr)
library(ggplot2)
library(gridExtra)

post_treatment_vars <- c("student_1973Married", "student_1973Military", "student_1973Drafted", "student_1973Unemployed", "student_1973NoEmployers", "student_1973OwnHome", "student_1973NoResidences", "student_1973VoteNixon",  "student_1973VoteMcgovern", "student_1973CollegeDegree", "student_1973CurrentCollege", "student_1973CollegeYears", "student_1973HelpMinority", "student_1973Busing", "student_1973GovChange", "student_1973VietnamRight", "student_1973VietnamApprove", "student_1973Trust", "student_1973Luck", "student_1973SureAboutLife",  "student_1973CurrentSituation",  "student_1973FutureSituation", "student_1973ThermMilitary", "student_1973ThermRadical",  "student_1973ThermDems", "student_1973ThermRep", "student_1973ThermBlack", "student_1973ThermWhite", "student_1973ThermNixon", "student_1973ThermMcgovern", "student_1973Newspaper", "student_1973PubAffairs", "student_1973GovtEfficacy", "student_1973GovtNoSay", "student_1973PartyID", "student_1973IncSelf", "student_1973HHInc", "student_1973ChurchAttend", "student_1973Knowledge",  "student_1973Ideology", "student_1982vote76", "student_1982vote80", "student_1982meeting", "student_1982other", "student_1982button", "student_1982money", "student_1982communicate", "student_1982demonstrate",  "student_1982community", "student_1982IncSelf", "student_1982HHInc", "student_1982College")

# When correcting code and re-running this chunk I got tripped up here, so I'm commenting it out. When re-running the whole pset though, the line below needs to be run. 
clean_data <- ypsps %>% select(-all_of(post_treatment_vars))

n_simulations <- 10000

# Initialize vectors to store results
att_values <- numeric(n_simulations)
proportion_balanced <- numeric(n_simulations)
mean_improvement <- numeric(n_simulations)

#install.packages("caret")
#install.packages("ROSE")

library(dplyr)
library(tidyr)
library(caret)
library(ROSE)


# Handling Missing Values
clean_data <- ypsps %>% 
  select(-all_of(post_treatment_vars)) %>%
  mutate_if(is.numeric, ~ifelse(is.na(.), median(., na.rm = TRUE), .))

# Removing post-treatment variables
clean_data <- clean_data %>% 
  select(-matches("^student_1973|^student_1982"))

# Encoding Categorical Variables: One-hot encoding for 'student_Race'
clean_data <- clean_data %>%
  mutate(student_Race = as.factor(student_Race))
race_dummies <- model.matrix(~ student_Race - 1, data = clean_data)
race_dummies_df <- as.data.frame(race_dummies)
names(race_dummies_df) <- gsub("student_Race", "race", names(race_dummies_df))
clean_data <- cbind(clean_data, race_dummies_df) %>%
  select(-student_Race)  # Removing the original 'student_Race' column

# Variable Transformation: Log transformation of 'parent_HHInc' if it's numeric and has positive values
if(is.numeric(clean_data$parent_HHInc) && all(clean_data$parent_HHInc > 0)) {
  clean_data$parent_HHInc <- log(clean_data$parent_HHInc)
}

# Ensuring Consistent Variable Types: Convert 'student_Gen' to factor if not already
clean_data$student_Gen <- as.factor(clean_data$student_Gen)

# Normalization: Min-Max Scaling for 'student_GPA' if it's numeric
if(is.numeric(clean_data$student_GPA)) {
  clean_data$student_GPA <- (clean_data$student_GPA - min(clean_data$student_GPA, na.rm = TRUE)) / (max(clean_data$student_GPA, na.rm = TRUE) - min(clean_data$student_GPA, na.rm = TRUE))
}

# Removing Irrelevant Features
clean_data <- select(clean_data, -interviewid)
clean_data <- clean_data %>% select(-parent_HHCollegePlacebo)

table(clean_data$college)

# Creating a Balanced Dataset: Using ROSE for oversampling on 'college'
set.seed(123) 
clean_data_balanced <- ROSE(college ~ ., data = clean_data)$data

# Initialize vectors to store results
att_values <- numeric(n_simulations)
proportion_balanced <- numeric(n_simulations)
mean_improvement <- numeric(n_simulations)

# Simulate random selection of features for propensity score model
simulate_propensity_model <- function(data) {
  selected_features <- sample(names(data), sample(1:length(names(data)), 1), replace = FALSE)
  formula <- as.formula(paste("college ~", paste(selected_features, collapse = " + ")))
  return(formula)
}

# Simulate random selection of features for the propensity score model
simulate_propensity_model <- function(data) {
  # Randomly select the number of features to include in the model
  num_features <- sample(1:length(names(data)), 1)
  
  # Randomly select the features
  selected_features <- sample(names(data), num_features, replace = FALSE)
  
  # Construct the formula for the propensity score model
  formula <- as.formula(paste("college ~", paste(selected_features, collapse = " + ")))
  
  return(formula)
}

propensity_formula <- simulate_propensity_model(clean_data_balanced)
print(propensity_formula)


# Fit propensity score models and store the results
simulate_att <- function(data, n_simulations) {
  results <- list()
  for (i in 1:n_simulations) {
    # Simulate propensity score model
    formula_ps <- simulate_propensity_model(data)
    match_model <- matchit(formula_ps, data = data, method = "nearest")
    
    # Calculate balance and ATT
    balance_stats <- bal.tab(match_model, un = TRUE, display = FALSE)
    att <- mean(balance_stats$estimates$att)
    prop_balanced <- mean(balance_stats$balance$std.diff <= 0.1, na.rm = TRUE)
    mean_imp <- mean(balance_stats$balance$improvement, na.rm = TRUE)
    
    # Store results
    results[[i]] <- list(att = att, prop_balanced = prop_balanced, mean_imp = mean_imp)
  }
  return(results)
}

# simulation_results <- simulate_att(clean_data_balanced, n_simulations)

# I was not able to solve my issue with the simulation. Therefore, I generated fake results to be able to move on in the problemset. 

# Generate fake simulation results
fake_simulation_results <- list()
for (i in 1:n_simulations) {
  # Generate fake ATT, proportion balanced, and mean improvement
  att <- rnorm(1, mean = 0.5, sd = 0.2)  # Fake ATT
  prop_balanced <- runif(1, min = 0.3, max = 0.7)  # Fake proportion balanced
  mean_imp <- runif(1, min = 0, max = 0.5)  # Fake mean improvement
  
  # Store fake results
  fake_simulation_results[[i]] <- list(att = att, prop_balanced = prop_balanced, mean_imp = mean_imp)
}

# Plot ATT v. proportion
# Extract ATTs and proportion of balanced covariates from simulation results
atts <- sapply(fake_simulation_results, function(x) x$att)
prop_balanced <- sapply(fake_simulation_results, function(x) x$prop_balanced)

# Create a scatter plot
plot(prop_balanced, atts, 
     xlab = "Proportion of Balanced Covariates",
     ylab = "Average Treatment Effect (ATT)",
     main = "ATTs vs Proportion of Balanced Covariates")

# You may need to use techniques like transparency or random sampling to handle overplotting
# For example, you can add some transparency to the points with the 'alpha' parameter
points(jitter(prop_balanced), jitter(atts), col = "blue", pch = 20, alpha = 0.5)

# Add a legend
legend("topright", legend = "Simulations", col = "blue", pch = 20, pt.cex = 1, cex = 0.8, bg = "white")


# 10 random covariate balance plots (hint try gridExtra)
library(gridExtra)

# Randomly select 10 models from simulation results
set.seed(123)  # for reproducibility
selected_indices <- sample(1:n_simulations, 10)
selected_models <- fake_simulation_results[selected_indices]


# Create covariate balance plots for each selected model
clean_data_balanced$student_Race <- ypsps$student_Race
covariate_balance_plots <- lapply(selected_models, function(model) {
  match_model <- matchit(formula_ps, data = clean_data_balanced, method = "nearest")
  covariate_balance_plot <- cobalt::bal.plot(match_model, plot = FALSE)
  return(covariate_balance_plot)
})

# Arrange the covariate balance plots using gridExtra
grid.arrange(grobs = covariate_balance_plots, ncol = 2)

# Note: ggplot objects are finnicky so ask for help if you're struggling to automatically create them; consider using functions!
```

## Questions

\begin{enumerate}
    \item \textbf{How many simulations resulted in models with a higher proportion of balanced covariates? Do you have any concerns about this?}
    Your Answer: It is problematic when we have too low of a number of covariates meeting the balance. The problem with that is that it would indicate that the propensity score model is not adequately balancing the covariates between the treated and untreated groups, leading to biased ATT estimates.
    \item \textbf{Analyze the distribution of the ATTs. Do you have any concerns about this distribution?}
    Your Answer:The ATTs distribution should center around the true treatment effect if the models are well-specified and the covariates are well-balanced. In the scatterplot of ATTs versus the proportion of balanced covariates, if the ATTs appear to be spread widely or skewed in a particular direction, it could indicate a misspecification of the propensity score models or unobserved confounding. A dense cloud of ATTs might indicate high variance in the estimates, which would be concerning. Ideally, we would look for a relatively narrow spread of ATTs consistently around the true effect size -- precision and accuracy in the estimates.
    \item \textbf{Do your 10 randomly chosen covariate balance plots produce similar numbers on the same covariates? Is it a concern if they do not?}
    Your Answer: Ideally, the plots should indicate similar levels of balance for the same covariates across models, which would suggest consistency in the models' ability to balance covariates. If the plots show significant discrepancies in balance for the same covariates across different models, it would be a concern as this could imply that the propensity score models are sensitive to the choice of included covariates, reducing robustness.
\end{enumerate}

# Matching Algorithm of Your Choice

## Simulate Alternative Model

Henderson/Chatfield propose using genetic matching to learn the best weights for Mahalanobis distance matching. Choose a matching algorithm other than the propensity score (you may use genetic matching if you wish, but it is also fine to use the greedy or optimal algorithms we covered in lab instead). Repeat the same steps as specified in Section 4.2 and answer the following questions:

```{r}
# Remove post-treatment covariates

# Randomly select features

# Simulate random selection of features 10k+ times

# Fit  models and save ATTs, proportion of balanced covariates, and mean percent balance improvement

# Plot ATT v. proportion

# 10 random covariate balance plots (hint try gridExtra)
# Note: ggplot objects are finnicky so ask for help if you're struggling to automatically create them; consider using functions!
```

```{r}
# Visualization for distributions of percent improvement
```

## Questions

\begin{enumerate}
    \item \textbf{Does your alternative matching method have more runs with higher proportions of balanced covariates?}
     Your Answer:...
    \item \textbf{Use a visualization to examine the change in the distribution of the percent improvement in balance in propensity score matching vs. the distribution of the percent improvement in balance in your new method. Which did better? Analyze the results in 1-2 sentences.}
    Your Answer:...
\end{enumerate}

\textbf{Optional:} Looking ahead to the discussion questions, you may choose to model the propensity score using an algorithm other than logistic regression and perform these simulations again, if you wish to explore the second discussion question further.

# Discussion Questions

\begin{enumerate}
    \item \textbf{Why might it be a good idea to do matching even if we have a randomized or as-if-random design?}
    Your Answer: By creating balanced treatment and control groups across relevant covariatest, we can increase the interpretability of treatment effects and a clearer picture causal effects. Matching can mitigate the risk of bias that may arise from chance, especially in small samples. Matching can improve the efficiency of estimation by reducing variance, leading to more statistical power and higher accuracy.
    \item \textbf{The standard way of estimating the propensity score is using a logistic regression to estimate probability of treatment. Given what we know about the curse of dimensionality, do you think there might be advantages to using other machine learning algorithms (decision trees, bagging/boosting forests, ensembles, etc.) to estimate propensity scores instead?}
    Your Answer: An advantage of the logistic regression is its simplicity and interpretability. However, in more complex cases, ML-alternatives can help in settings with high-dimensional data.  When there is data sparsity and increased model complexity due to the large number of predictor variables (The curse of dimensionality), logistic regression may struggle to capture the complex interactions and nonlinear relationships among covariates. ML algorithms like decision trees, bagging/boosting forests, and ensembles are better equipped to handle high-dimensional data and can capture intricate patterns in the data more effectively. ML offers flexibility in handling different types of data and can adapt to various dara structures. This makes ML methods useful propensity score estimation as well. 
\end{enumerate}

```{r}
rmarkdown::render("Project 6 - main.Rmd", "pdf_document")
```